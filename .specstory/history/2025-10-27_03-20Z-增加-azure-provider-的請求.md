<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session f9bb0112-b202-4d61-8e40-fe2827129a07 (2025-10-27 03:20Z) -->

# 增加 Azure provider 的請求 (2025-10-27 03:20Z)

_**User (2025-10-27 03:20Z)**_

請參考@openai.py 增加一個 Azure 的 provider (azure.py)



---

_**Agent (model default, mode Agent)**_



---

檢查專案現有結構與 Azure 相關程式碼。


---

<tool-use data-tool-type="search" data-tool-name="codebase_search">
<details>
                <summary>Tool use: **codebase_search** • Searched codebase "Azure OpenAI provider implementation" • **12** results</summary>
            
| File | Lines |
|------|-------|
| `langextract/providers/openai.py` | L29-267 |
| `langextract/providers/openai.py` | L28-267 |
| `langextract/providers/openai.py` | L267 |
| `tests/factory_test.py` | L33-420 |
| `langextract/providers/patterns.py` | L34 |
| `eval/demo_azure_fix.py` | L21 |
| `README.md` | L247-304 |
| `langextract/providers/README.md` | L47-106 |
| `langextract/providers/README.md` | L492-556 |
| `langextract/providers/builtin_registry.py` | L25-50 |
| `langextract/factory.py` | L33-161 |
| `tests/inference_test.py` | L523-788 |

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/providers/patterns.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/core/base_model.py</summary>

</details>
</tool-use>

---



---

檢查 Azure 相關檔案與現有註冊：


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: eval/demo_azure_fix.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/providers/__init__.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/providers/router.py</summary>

</details>
</tool-use>

---



---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/providers/builtin_registry.py</summary>

</details>
</tool-use>

---



---

參考 openai.py，準備建立 azure.py，並更新 patterns 與 builtin_registry。


---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [-] 創建 azure.py provider 檔案
- [ ] 在 patterns.py 中添加 Azure 模式定義
- [ ] 在 builtin_registry.py 中註冊 Azure provider
- [ ] 更新 __init__.py 匯出 azure 模組

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** • Edit file: langextract/providers/azure.py</summary>

**Chunk 1**
Lines added: 272, lines removed: 1

```diff
@@ -1,1 +1,272 @@
- 
+ # Copyright 2025 Google LLC.
+ #
+ # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
+ # you may not use this file except in compliance with the License.
+ # You may obtain a copy of the License at
+ #
+ #     http://www.apache.org/licenses/LICENSE-2.0
+ #
+ # Unless required by applicable law or agreed to in writing, software
+ # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ # See the License for the specific language governing permissions and
+ # limitations under the License.
+ 
+ &quot;&quot;&quot;Azure OpenAI provider for LangExtract.&quot;&quot;&quot;
+ # pylint: disable=duplicate-code
+ 
+ from __future__ import annotations
+ 
+ import concurrent.futures
+ import dataclasses
+ from typing import Any, Iterator, Sequence
+ 
+ from langextract.core import base_model
+ from langextract.core import data
+ from langextract.core import exceptions
+ from langextract.core import schema
+ from langextract.core import types as core_types
+ from langextract.providers import patterns
+ from langextract.providers import router
+ 
+ 
+ @router.register(
+     *patterns.AZURE_PATTERNS,
+     priority=patterns.AZURE_PRIORITY,
+ )
+ @dataclasses.dataclass(init=False)
+ class AzureLanguageModel(base_model.BaseLanguageModel):
+   &quot;&quot;&quot;Language model inference using Azure OpenAI&#39;s API with structured output.&quot;&quot;&quot;
+ 
+   deployment_name: str = &#39;gpt-4o-mini&#39;
+   api_key: str | None = None
+   endpoint: str | None = None
+   api_version: str = &#39;2024-02-15-preview&#39;
+   format_type: data.FormatType = data.FormatType.JSON
+   temperature: float | None = None
+   max_workers: int = 10
+   _client: Any = dataclasses.field(default=None, repr=False, compare=False)
+   _extra_kwargs: dict[str, Any] = dataclasses.field(
+       default_factory=dict, repr=False, compare=False
+   )
+ 
+   @property
+   def requires_fence_output(self) -&gt; bool:
+     &quot;&quot;&quot;Azure OpenAI JSON mode returns raw JSON without fences.&quot;&quot;&quot;
+     if self.format_type == data.FormatType.JSON:
+       return False
+     return super().requires_fence_output
+ 
+   def __init__(
+       self,
+       deployment_name: str = &#39;gpt-4o-mini&#39;,
+       api_key: str | None = None,
+       endpoint: str | None = None,
+       api_version: str = &#39;2024-02-15-preview&#39;,
+       format_type: data.FormatType = data.FormatType.JSON,
+       temperature: float | None = None,
+       max_workers: int = 10,
+       **kwargs,
+   ) -&gt; None:
+     &quot;&quot;&quot;Initialize the Azure OpenAI language model.
+ 
+     Args:
+       deployment_name: The Azure deployment name to use (e.g., &#39;gpt-4o-mini&#39;).
+       api_key: API key for Azure OpenAI service.
+       endpoint: Azure OpenAI endpoint URL (e.g.,
+         &#39;https://your-resource.openai.azure.com&#39;).
+       api_version: Azure OpenAI API version.
+       format_type: Output format (JSON or YAML).
+       temperature: Sampling temperature.
+       max_workers: Maximum number of parallel API calls.
+       **kwargs: Ignored extra parameters so callers can pass a superset of
+         arguments shared across back-ends without raising &#96;&#96;TypeError&#96;&#96;.
+     &quot;&quot;&quot;
+     # Lazy import: OpenAI package required
+     try:
+       # pylint: disable=import-outside-toplevel
+       import openai
+     except ImportError as e:
+       raise exceptions.InferenceConfigError(
+           &#39;Azure provider requires openai package. &#39;
+           &#39;Install with: pip install langextract[openai]&#39;
+       ) from e
+ 
+     self.deployment_name = deployment_name
+     self.api_key = api_key
+     self.endpoint = endpoint
+     self.api_version = api_version
+     self.format_type = format_type
+     self.temperature = temperature
+     self.max_workers = max_workers
+ 
+     if not self.api_key:
+       raise exceptions.InferenceConfigError(&#39;API key not provided.&#39;)
+     if not self.endpoint:
+       raise exceptions.InferenceConfigError(&#39;Endpoint not provided.&#39;)
+ 
+     # Initialize the Azure OpenAI client
+     self._client = openai.AzureOpenAI(
+         api_key=self.api_key,
+         azure_endpoint=self.endpoint,
+         api_version=self.api_version,
+     )
+ 
+     super().__init__(
+         constraint=schema.Constraint(constraint_type=schema.ConstraintType.NONE)
+     )
+     self._extra_kwargs = kwargs or {}
+ 
+   def _normalize_reasoning_params(self, config: dict) -&gt; dict:
+     &quot;&quot;&quot;Normalize reasoning parameters for API compatibility.
+ 
+     Converts flat &#39;reasoning_effort&#39; to nested &#39;reasoning&#39; structure.
+     Merges with existing reasoning dict if present.
+     &quot;&quot;&quot;
+     result = config.copy()
+ 
+     if &#39;reasoning_effort&#39; in result:
+       effort = result.pop(&#39;reasoning_effort&#39;)
+       reasoning = result.get(&#39;reasoning&#39;, {}) or {}
+       reasoning.setdefault(&#39;effort&#39;, effort)
+       result[&#39;reasoning&#39;] = reasoning
+ 
+     return result
+ 
+   def _process_single_prompt(
+       self, prompt: str, config: dict
+   ) -&gt; core_types.ScoredOutput:
+     &quot;&quot;&quot;Process a single prompt and return a ScoredOutput.&quot;&quot;&quot;
+     try:
+       normalized_config = self._normalize_reasoning_params(config)
+ 
+       system_message = &#39;&#39;
+       if self.format_type == data.FormatType.JSON:
+         system_message = (
+             &#39;You are a helpful assistant that responds in JSON format.&#39;
+         )
+       elif self.format_type == data.FormatType.YAML:
+         system_message = (
+             &#39;You are a helpful assistant that responds in YAML format.&#39;
+         )
+ 
+       messages = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: prompt}]
+       if system_message:
+         messages.insert(0, {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: system_message})
+ 
+       api_params = {
+           &#39;model&#39;: self.deployment_name,
+           &#39;messages&#39;: messages,
+           &#39;n&#39;: 1,
+       }
+ 
+       temp = normalized_config.get(&#39;temperature&#39;, self.temperature)
+       if temp is not None:
+         api_params[&#39;temperature&#39;] = temp
+ 
+       if self.format_type == data.FormatType.JSON:
+         api_params.setdefault(&#39;response_format&#39;, {&#39;type&#39;: &#39;json_object&#39;})
+ 
+       if (v := normalized_config.get(&#39;max_output_tokens&#39;)) is not None:
+         api_params[&#39;max_tokens&#39;] = v
+       if (v := normalized_config.get(&#39;top_p&#39;)) is not None:
+         api_params[&#39;top_p&#39;] = v
+       for key in [
+           &#39;frequency_penalty&#39;,
+           &#39;presence_penalty&#39;,
+           &#39;seed&#39;,
+           &#39;stop&#39;,
+           &#39;logprobs&#39;,
+           &#39;top_logprobs&#39;,
+           &#39;reasoning&#39;,
+           &#39;response_format&#39;,
+       ]:
+         if (v := normalized_config.get(key)) is not None:
+           api_params[key] = v
+ 
+       response = self._client.chat.completions.create(**api_params)
+ 
+       # Extract the response text using the v1.x response format
+       output_text = response.choices[0].message.content
+ 
+       return core_types.ScoredOutput(score=1.0, output=output_text)
+ 
+     except Exception as e:
+       raise exceptions.InferenceRuntimeError(
+           f&#39;Azure OpenAI API error: {str(e)}&#39;, original=e
+       ) from e
+ 
+   def infer(
+       self, batch_prompts: Sequence[str], **kwargs
+   ) -&gt; Iterator[Sequence[core_types.ScoredOutput]]:
+     &quot;&quot;&quot;Runs inference on a list of prompts via Azure OpenAI&#39;s API.
+ 
+     Args:
+       batch_prompts: A list of string prompts.
+       **kwargs: Additional generation params (temperature, top_p, etc.)
+ 
+     Yields:
+       Lists of ScoredOutputs.
+     &quot;&quot;&quot;
+     merged_kwargs = self.merge_kwargs(kwargs)
+ 
+     config = {}
+ 
+     temp = merged_kwargs.get(&#39;temperature&#39;, self.temperature)
+     if temp is not None:
+       config[&#39;temperature&#39;] = temp
+     if &#39;max_output_tokens&#39; in merged_kwargs:
+       config[&#39;max_output_tokens&#39;] = merged_kwargs[&#39;max_output_tokens&#39;]
+     if &#39;top_p&#39; in merged_kwargs:
+       config[&#39;top_p&#39;] = merged_kwargs[&#39;top_p&#39;]
+ 
+     for key in [
+         &#39;frequency_penalty&#39;,
+         &#39;presence_penalty&#39;,
+         &#39;seed&#39;,
+         &#39;stop&#39;,
+         &#39;logprobs&#39;,
+         &#39;top_logprobs&#39;,
+         &#39;reasoning_effort&#39;,
+         &#39;reasoning&#39;,
+         &#39;response_format&#39;,
+     ]:
+       if key in merged_kwargs:
+         config[key] = merged_kwargs[key]
+ 
+     # Use parallel processing for batches larger than 1
+     if len(batch_prompts) &gt; 1 and self.max_workers &gt; 1:
+       with concurrent.futures.ThreadPoolExecutor(
+           max_workers=min(self.max_workers, len(batch_prompts))
+       ) as executor:
+         future_to_index = {
+             executor.submit(
+                 self._process_single_prompt, prompt, config.copy()
+             ): i
+             for i, prompt in enumerate(batch_prompts)
+         }
+ 
+         results: list[core_types.ScoredOutput | None] = [None] * len(
+             batch_prompts
+         )
+         for future in concurrent.futures.as_completed(future_to_index):
+           index = future_to_index[future]
+           try:
+             results[index] = future.result()
+           except Exception as e:
+             raise exceptions.InferenceRuntimeError(
+                 f&#39;Parallel inference error: {str(e)}&#39;, original=e
+             ) from e
+ 
+         for result in results:
+           if result is None:
+             raise exceptions.InferenceRuntimeError(
+                 &#39;Failed to process one or more prompts&#39;
+             )
+           yield [result]
+     else:
+       # Sequential processing for single prompt or worker
+       for prompt in batch_prompts:
+         result = self._process_single_prompt(prompt, config.copy())
+         yield [result]  # pylint: disable=duplicate-code
+ 
```

</details>

</tool-use>

---



---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] 創建 azure.py provider 檔案
- [-] 在 patterns.py 中添加 Azure 模式定義
- [ ] 在 builtin_registry.py 中註冊 Azure provider
- [ ] 更新 __init__.py 匯出 azure 模組

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/providers/patterns.py</summary>

</details>
</tool-use>

---



---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: langextract/providers/patterns.py</summary>

**Chunk 1**
Lines added: 9, lines removed: undefined

```diff
@@ -31,4 +31,13 @@
  )
  OPENAI_PRIORITY = 10
+ 
+ # Azure OpenAI provider patterns
+ AZURE_PATTERNS = (
+     r&#39;^azure&#39;,  # azure deployment names
+     r&#39;^gpt-4o-mini-azure&#39;,  # specific azure model names
+     r&#39;^gpt-4o-azure&#39;,
+     r&#39;^gpt-35-turbo-azure&#39;,
+ )
+ AZURE_PRIORITY = 15  # Higher priority than OpenAI to handle Azure-specific configs
  
  # Ollama provider patterns
```

</details>

</tool-use>

---



---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] 創建 azure.py provider 檔案
- [x] 在 patterns.py 中添加 Azure 模式定義
- [-] 在 builtin_registry.py 中註冊 Azure provider
- [ ] 更新 __init__.py 匯出 azure 模組

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: langextract/providers/builtin_registry.py</summary>

**Chunk 1**
Lines added: 5, lines removed: undefined

```diff
@@ -39,4 +39,9 @@
          &#39;priority&#39;: patterns.GEMINI_PRIORITY,
      },
+     {
+         &#39;patterns&#39;: patterns.AZURE_PATTERNS,
+         &#39;target&#39;: &#39;langextract.providers.azure:AzureLanguageModel&#39;,
+         &#39;priority&#39;: patterns.AZURE_PRIORITY,
+     },
      {
          &#39;patterns&#39;: patterns.OLLAMA_PATTERNS,
```

</details>

</tool-use>

---



---

檢查並更新 `__init__.py` 以匯出 azure 模組：


---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] 創建 azure.py provider 檔案
- [x] 在 patterns.py 中添加 Azure 模式定義
- [x] 在 builtin_registry.py 中註冊 Azure provider
- [-] 更新 __init__.py 匯出 azure 模組

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **code_edit** • Edit file: langextract/providers/__init__.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -34,4 +34,5 @@
  
  __all__ = [
+     &#39;azure&#39;,
      &#39;gemini&#39;,
      &#39;openai&#39;,
```

</details>

</tool-use>

---



---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: langextract/providers/__init__.py</summary>

</details>
</tool-use>

---



---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] 創建 azure.py provider 檔案
- [x] 在 patterns.py 中添加 Azure 模式定義
- [x] 在 builtin_registry.py 中註冊 Azure provider
- [x] 更新 __init__.py 匯出 azure 模組

</details>
</tool-use>

---



---

檢查 lint 錯誤：


---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 4 files</summary>
      
Lint paths:

- `langextract/providers/azure.py`
- `langextract/providers/patterns.py`
- `langextract/providers/builtin_registry.py`
- `langextract/providers/__init__.py`

Lint results


**No lint errors found**
</details>
</tool-use>

---



---

已新增 Azure provider。變更如下：

## 完成的工作

### 1. **建立 `langextract/providers/azure.py`**
   - 以 `OpenAILanguageModel` 為基礎
   - 包含 `api_key`、`endpoint`、`api_version`
   - 支援並行處理
   - 支援 JSON 與 YAML
   - 使用 `openai.AzureOpenAI` 初始化客戶端

### 2. **更新 `langextract/providers/patterns.py`**
   - 新增 `AZURE_PATTERNS` 以識別 Azure 部署名稱
   - 設定 `AZURE_PRIORITY = 15`

### 3. **更新 `langextract/providers/builtin_registry.py`**
   - 註冊 `AzureLanguageModel`

### 4. **更新 `langextract/providers/__init__.py`**
   - 在 `__all__` 加入 `'azure'`

Azure provider 可透過 `factory.create_model` 使用，或直接實例化 `AzureLanguageModel`。

範例：
```python
import langextract as lx

config = lx.factory.ModelConfig(
    model_id="azure",
    provider="AzureLanguageModel",
    provider_kwargs={
        "api_key": "your-azure-api-key",
        "endpoint": "https://your-resource.openai.azure.com",
        "deployment_name": "gpt-4o-mini"
    }
)
model = lx.factory.create_model(config)
```

---

